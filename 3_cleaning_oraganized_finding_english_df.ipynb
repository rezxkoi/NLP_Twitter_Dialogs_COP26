{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1005c3d6",
   "metadata": {},
   "source": [
    "# cleaning oraganized dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d981a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9aa197",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('organized_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4306e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>quoteCount</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>twitter_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-04 23:59:50+00:00</td>\n",
       "      <td>@Banterm42404691 @MrHarryCole Absolutely one s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['COP26']</td>\n",
       "      <td>aquitainexox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-11-04 23:59:48+00:00</td>\n",
       "      <td>Please join us at a #COP26 side event \"Impleme...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['COP26']</td>\n",
       "      <td>GEC_JCM_Info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-11-04 23:59:46+00:00</td>\n",
       "      <td>#India‚Äôs huge solar uptake has boosted #climat...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['India', 'climate', 'COP26']</td>\n",
       "      <td>zpnine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-11-04 23:59:39+00:00</td>\n",
       "      <td>Reuni√≥n #COP26 Reuni√≥n mundial de pa√≠ses sobre...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['COP26', 'AcuerdoDeParis', 'LenguajeClaro']</td>\n",
       "      <td>AsoPalabrasCl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-11-04 23:59:35+00:00</td>\n",
       "      <td>@_pln_id PLN terdepan dukung pemanfaatan EBTüçÉ\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['COP26', 'PLNGreen']</td>\n",
       "      <td>Crstianst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>2021-11-04 13:20:36+00:00</td>\n",
       "      <td>ClimateChange is no more only a challenge, it‚Äô...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>['Climate', 'ClimateAction', 'COP26', 'climate...</td>\n",
       "      <td>AqsaYounasRana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>2021-11-04 13:20:18+00:00</td>\n",
       "      <td>We confronted the coal baron this morning! Fuc...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>['JoeManchin', 'JoeManchinSenatorForSale', 'Jo...</td>\n",
       "      <td>pete4peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7613</th>\n",
       "      <td>2021-11-04 13:19:58+00:00</td>\n",
       "      <td>Feeling simply disgusted by countries such as ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['china', 'usa', 'india', 'COP26Glasgow', 'Fos...</td>\n",
       "      <td>mmill_landscape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7614</th>\n",
       "      <td>2021-11-04 13:19:45+00:00</td>\n",
       "      <td>üö® Join us in calling for @Google and #Facebook...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['Facebook', 'climatechange', 'disinformation'...</td>\n",
       "      <td>KarynCaplan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7615</th>\n",
       "      <td>2021-11-04 13:19:25+00:00</td>\n",
       "      <td>There is no doubt! \"The science is clear: to k...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['COP26Glasgow', 'COP26', 'ClimateActionNow', ...</td>\n",
       "      <td>DrDuly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7616 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date  \\\n",
       "0     2021-11-04 23:59:50+00:00   \n",
       "1     2021-11-04 23:59:48+00:00   \n",
       "2     2021-11-04 23:59:46+00:00   \n",
       "3     2021-11-04 23:59:39+00:00   \n",
       "4     2021-11-04 23:59:35+00:00   \n",
       "...                         ...   \n",
       "7611  2021-11-04 13:20:36+00:00   \n",
       "7612  2021-11-04 13:20:18+00:00   \n",
       "7613  2021-11-04 13:19:58+00:00   \n",
       "7614  2021-11-04 13:19:45+00:00   \n",
       "7615  2021-11-04 13:19:25+00:00   \n",
       "\n",
       "                                                content  likeCount  \\\n",
       "0     @Banterm42404691 @MrHarryCole Absolutely one s...          0   \n",
       "1     Please join us at a #COP26 side event \"Impleme...          2   \n",
       "2     #India‚Äôs huge solar uptake has boosted #climat...          1   \n",
       "3     Reuni√≥n #COP26 Reuni√≥n mundial de pa√≠ses sobre...          0   \n",
       "4     @_pln_id PLN terdepan dukung pemanfaatan EBTüçÉ\\...          0   \n",
       "...                                                 ...        ...   \n",
       "7611  ClimateChange is no more only a challenge, it‚Äô...          5   \n",
       "7612  We confronted the coal baron this morning! Fuc...          3   \n",
       "7613  Feeling simply disgusted by countries such as ...          0   \n",
       "7614  üö® Join us in calling for @Google and #Facebook...          0   \n",
       "7615  There is no doubt! \"The science is clear: to k...          0   \n",
       "\n",
       "      quoteCount                                           hashtags  \\\n",
       "0              0                                          ['COP26']   \n",
       "1              1                                          ['COP26']   \n",
       "2              0                      ['India', 'climate', 'COP26']   \n",
       "3              0       ['COP26', 'AcuerdoDeParis', 'LenguajeClaro']   \n",
       "4              0                              ['COP26', 'PLNGreen']   \n",
       "...          ...                                                ...   \n",
       "7611           0  ['Climate', 'ClimateAction', 'COP26', 'climate...   \n",
       "7612           0  ['JoeManchin', 'JoeManchinSenatorForSale', 'Jo...   \n",
       "7613           0  ['china', 'usa', 'india', 'COP26Glasgow', 'Fos...   \n",
       "7614           0  ['Facebook', 'climatechange', 'disinformation'...   \n",
       "7615           0  ['COP26Glasgow', 'COP26', 'ClimateActionNow', ...   \n",
       "\n",
       "           twitter_id  \n",
       "0        aquitainexox  \n",
       "1        GEC_JCM_Info  \n",
       "2              zpnine  \n",
       "3       AsoPalabrasCl  \n",
       "4           Crstianst  \n",
       "...               ...  \n",
       "7611   AqsaYounasRana  \n",
       "7612       pete4peace  \n",
       "7613  mmill_landscape  \n",
       "7614      KarynCaplan  \n",
       "7615           DrDuly  \n",
       "\n",
       "[7616 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a93f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7616 entries, 0 to 7615\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   date        7616 non-null   object\n",
      " 1   content     7616 non-null   object\n",
      " 2   likeCount   7616 non-null   int64 \n",
      " 3   quoteCount  7616 non-null   int64 \n",
      " 4   hashtags    7616 non-null   object\n",
      " 5   twitter_id  7616 non-null   object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 357.1+ KB\n"
     ]
    }
   ],
   "source": [
    "main_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffeff5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-6d528d888e84>:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  main_df.cp = main_df.copy()\n"
     ]
    }
   ],
   "source": [
    "main_df.cp = main_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "925a8983",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tweet_df = main_df.cp[['twitter_id','content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "397ff2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitter_id</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aquitainexox</td>\n",
       "      <td>@Banterm42404691 @MrHarryCole Absolutely one s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GEC_JCM_Info</td>\n",
       "      <td>Please join us at a #COP26 side event \"Impleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zpnine</td>\n",
       "      <td>#India‚Äôs huge solar uptake has boosted #climat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AsoPalabrasCl</td>\n",
       "      <td>Reuni√≥n #COP26 Reuni√≥n mundial de pa√≠ses sobre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crstianst</td>\n",
       "      <td>@_pln_id PLN terdepan dukung pemanfaatan EBTüçÉ\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>AqsaYounasRana</td>\n",
       "      <td>ClimateChange is no more only a challenge, it‚Äô...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>pete4peace</td>\n",
       "      <td>We confronted the coal baron this morning! Fuc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7613</th>\n",
       "      <td>mmill_landscape</td>\n",
       "      <td>Feeling simply disgusted by countries such as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7614</th>\n",
       "      <td>KarynCaplan</td>\n",
       "      <td>üö® Join us in calling for @Google and #Facebook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7615</th>\n",
       "      <td>DrDuly</td>\n",
       "      <td>There is no doubt! \"The science is clear: to k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7616 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           twitter_id                                            content\n",
       "0        aquitainexox  @Banterm42404691 @MrHarryCole Absolutely one s...\n",
       "1        GEC_JCM_Info  Please join us at a #COP26 side event \"Impleme...\n",
       "2              zpnine  #India‚Äôs huge solar uptake has boosted #climat...\n",
       "3       AsoPalabrasCl  Reuni√≥n #COP26 Reuni√≥n mundial de pa√≠ses sobre...\n",
       "4           Crstianst  @_pln_id PLN terdepan dukung pemanfaatan EBTüçÉ\\...\n",
       "...               ...                                                ...\n",
       "7611   AqsaYounasRana  ClimateChange is no more only a challenge, it‚Äô...\n",
       "7612       pete4peace  We confronted the coal baron this morning! Fuc...\n",
       "7613  mmill_landscape  Feeling simply disgusted by countries such as ...\n",
       "7614      KarynCaplan  üö® Join us in calling for @Google and #Facebook...\n",
       "7615           DrDuly  There is no doubt! \"The science is clear: to k...\n",
       "\n",
       "[7616 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tweet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dbd77c",
   "metadata": {},
   "source": [
    "## Appying cleaning fucntions to the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fec88d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp38-cp38-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24.0 MB 30.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58 kB 7.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /Users/amirreza/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/amirreza/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.20.1)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.1.2 smart-open-5.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f3a1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@'         # define a string of punctuation symbols\n",
    "\n",
    "# Functions to clean tweets\n",
    "def remove_links(tweet):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)   # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)  # remove bitly links\n",
    "    tweet = tweet.strip('[link]')   # remove [links]\n",
    "    tweet = re.sub(r'pic.twitter\\S+','', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove re-tweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "def remove_hashtags(tweet):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove hash tags\n",
    "    return tweet\n",
    "\n",
    "def remove_av(tweet):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    tweet = re.sub('VIDEO:', '', tweet)  # remove 'VIDEO:' from start of tweet\n",
    "    tweet = re.sub('AUDIO:', '', tweet)  # remove 'AUDIO:' from start of tweet\n",
    "    return tweet\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Returns tokenized representation of words in lemma form excluding stopwords\"\"\"\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(tweet):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS \\\n",
    "                and len(token) > 2:  # drops words with less than 3 characters\n",
    "            result.append(lemmatize(token))\n",
    "    return result\n",
    "\n",
    "def lemmatize(token):\n",
    "    \"\"\"Returns lemmatization of a token\"\"\"\n",
    "    return WordNetLemmatizer().lemmatize(token, pos='v')\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"Main master function to clean tweets, stripping noisy characters, and tokenizing use lemmatization\"\"\"\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_hashtags(tweet)\n",
    "    tweet = remove_av(tweet)\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    tweet_token_list = tokenize(tweet)  # apply lemmatization and tokenization\n",
    "    tweet = ' '.join(tweet_token_list)\n",
    "    return tweet\n",
    "\n",
    "def basic_clean(tweet):\n",
    "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_hashtags(tweet)\n",
    "    tweet = remove_av(tweet)\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    tweet = re.sub('üìù ‚Ä¶', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def tokenize_tweets(df):\n",
    "    \"\"\"Main function to read in and return cleaned and preprocessed dataframe.\n",
    "    This can be used in Jupyter notebooks by importing this module and calling the tokenize_tweets() function\n",
    "    Args:\n",
    "        df = data frame object to apply cleaning to\n",
    "    Returns:\n",
    "        pandas data frame with cleaned tokens\n",
    "    \"\"\"\n",
    "\n",
    "    df['tokens'] = df.tweet.apply(preprocess_tweet)\n",
    "    num_tweets = len(df)\n",
    "    print('Complete. Number of Tweets that have been cleaned and tokenized : {}'.format(num_tweets))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "374302da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-53f7133d7486>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  id_tweet_df['removed_links'] = removed_links\n"
     ]
    }
   ],
   "source": [
    "#id_tweet_df\n",
    "removed_links = []\n",
    "for i in range(len(id_tweet_df['content'])):\n",
    "    removed_links.append(remove_links(id_tweet_df['content'][i]))\n",
    "id_tweet_df['removed_links'] = removed_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac315d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Banterm42404691 @MrHarryCole Absolutely one self indulgent ego trip!! #COP26'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tweet_df['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43a15827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Banterm42404691 @MrHarryCole Absolutely one self indulgent ego trip!! #COP26'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tweet_df['removed_links'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3062edee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-04bce3b4546d>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  id_tweet_df['removed_users'] = removed_users\n"
     ]
    }
   ],
   "source": [
    "removed_users = []\n",
    "for i in range(len(id_tweet_df['content'])):\n",
    "    removed_users.append(remove_users(id_tweet_df['removed_links'][i]))\n",
    "id_tweet_df['removed_users'] = removed_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6a914cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Banterm42404691 @MrHarryCole Absolutely one self indulgent ego trip!! #COP26'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tweet_df['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c66ee14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Absolutely one self indulgent ego trip!! #COP26'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tweet_df['removed_users'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01d31499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-3b49b71db1ab>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  id_tweet_df['removed_hashtags'] = removed_hashtags\n"
     ]
    }
   ],
   "source": [
    "removed_hashtags = []\n",
    "for i in range(len(id_tweet_df['content'])):\n",
    "    removed_hashtags.append(remove_hashtags(id_tweet_df['removed_users'][i]))\n",
    "id_tweet_df['removed_hashtags'] = removed_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92fef9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Absolutely one self indulgent ego trip!! '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tweet_df['removed_hashtags'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2b68e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_av = []\n",
    "for i in range(len(id_tweet_df['content'])):\n",
    "    removed_av.append(remove_av(id_tweet_df['removed_hashtags'][i]))\n",
    "id_tweet_df['removed_av'] = removed_av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dac5482e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Absolutely one self indulgent ego trip!! '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tweet_df['removed_av'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6954a8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in range(len(id_tweet_df['content'])):\n",
    "    tokens.append(preprocess_tweet(id_tweet_df['content'][i]))\n",
    "id_tweet_df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "065bb664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'absolutely self indulgent ego trip'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tweet_df['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee4f3de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = []\n",
    "for i in range(len(id_tweet_df['tokens'])):\n",
    "    lemmatized.append(lemmatize(id_tweet_df['tokens'][i]))\n",
    "id_tweet_df['lemmatized'] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c653967f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'absolutely self indulgent ego trip'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tweet_df['lemmatized'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "085f42c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def preprocess(text):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        \n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            \n",
    "            # TODO: Apply lemmatize_stemming() on the token, then add to the results list\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    \n",
    "    return result'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5fbb121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess(list(id_tweet_df['removed_av']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cea883d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"preprocess = []\\nfor i in range(len(id_tweet_df['lemmatized'])):\\n    preprocess.append(preprocess(id_tweet_df['lemmatized'][i]))\\nid_tweet_df['preprocess'] = preprocess\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''preprocess = []\n",
    "for i in range(len(id_tweet_df['lemmatized'])):\n",
    "    preprocess.append(preprocess(id_tweet_df['lemmatized'][i]))\n",
    "id_tweet_df['preprocess'] = preprocess'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e99f6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitter_id</th>\n",
       "      <th>content</th>\n",
       "      <th>removed_links</th>\n",
       "      <th>removed_users</th>\n",
       "      <th>removed_hashtags</th>\n",
       "      <th>removed_av</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aquitainexox</td>\n",
       "      <td>@Banterm42404691 @MrHarryCole Absolutely one s...</td>\n",
       "      <td>@Banterm42404691 @MrHarryCole Absolutely one s...</td>\n",
       "      <td>Absolutely one self indulgent ego trip!! #COP26</td>\n",
       "      <td>Absolutely one self indulgent ego trip!!</td>\n",
       "      <td>Absolutely one self indulgent ego trip!!</td>\n",
       "      <td>absolutely self indulgent ego trip</td>\n",
       "      <td>absolutely self indulgent ego trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GEC_JCM_Info</td>\n",
       "      <td>Please join us at a #COP26 side event \"Impleme...</td>\n",
       "      <td>Please join us at a #COP26 side event \"Impleme...</td>\n",
       "      <td>Please join us at a #COP26 side event \"Impleme...</td>\n",
       "      <td>Please join us at a  side event \"Implementing ...</td>\n",
       "      <td>Please join us at a  side event \"Implementing ...</td>\n",
       "      <td>join event implement joint credit mechanism jc...</td>\n",
       "      <td>join event implement joint credit mechanism jc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zpnine</td>\n",
       "      <td>#India‚Äôs huge solar uptake has boosted #climat...</td>\n",
       "      <td>#India‚Äôs huge solar uptake has boosted #climat...</td>\n",
       "      <td>#India‚Äôs huge solar uptake has boosted #climat...</td>\n",
       "      <td>‚Äôs huge solar uptake has boosted  goals, says ...</td>\n",
       "      <td>‚Äôs huge solar uptake has boosted  goals, says ...</td>\n",
       "      <td>huge solar uptake boost goals say minister</td>\n",
       "      <td>huge solar uptake boost goals say minister</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AsoPalabrasCl</td>\n",
       "      <td>Reuni√≥n #COP26 Reuni√≥n mundial de pa√≠ses sobre...</td>\n",
       "      <td>Reuni√≥n #COP26 Reuni√≥n mundial de pa√≠ses sobre...</td>\n",
       "      <td>Reuni√≥n #COP26 Reuni√≥n mundial de pa√≠ses sobre...</td>\n",
       "      <td>Reuni√≥n  Reuni√≥n mundial de pa√≠ses sobre cambi...</td>\n",
       "      <td>Reuni√≥n  Reuni√≥n mundial de pa√≠ses sobre cambi...</td>\n",
       "      <td>reuni√≥n reuni√≥n mundial pa√≠ses sobre cambio cl...</td>\n",
       "      <td>reuni√≥n reuni√≥n mundial pa√≠ses sobre cambio cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crstianst</td>\n",
       "      <td>@_pln_id PLN terdepan dukung pemanfaatan EBTüçÉ\\...</td>\n",
       "      <td>@_pln_id PLN terdepan dukung pemanfaatan EBTüçÉ\\...</td>\n",
       "      <td>@_pln_id PLN terdepan dukung pemanfaatan EBTüçÉ\\...</td>\n",
       "      <td>@_pln_id PLN terdepan dukung pemanfaatan EBTüçÉ\\...</td>\n",
       "      <td>@_pln_id PLN terdepan dukung pemanfaatan EBTüçÉ\\...</td>\n",
       "      <td>pln pln terdepan dukung pemanfaatan ebt</td>\n",
       "      <td>pln pln terdepan dukung pemanfaatan ebt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      twitter_id                                            content  \\\n",
       "0   aquitainexox  @Banterm42404691 @MrHarryCole Absolutely one s...   \n",
       "1   GEC_JCM_Info  Please join us at a #COP26 side event \"Impleme...   \n",
       "2         zpnine  #India‚Äôs huge solar uptake has boosted #climat...   \n",
       "3  AsoPalabrasCl  Reuni√≥n #COP26 Reuni√≥n mundial de pa√≠ses sobre...   \n",
       "4      Crstianst  @_pln_id PLN terdepan dukung pemanfaatan EBTüçÉ\\...   \n",
       "\n",
       "                                       removed_links  \\\n",
       "0  @Banterm42404691 @MrHarryCole Absolutely one s...   \n",
       "1  Please join us at a #COP26 side event \"Impleme...   \n",
       "2  #India‚Äôs huge solar uptake has boosted #climat...   \n",
       "3  Reuni√≥n #COP26 Reuni√≥n mundial de pa√≠ses sobre...   \n",
       "4  @_pln_id PLN terdepan dukung pemanfaatan EBTüçÉ\\...   \n",
       "\n",
       "                                       removed_users  \\\n",
       "0    Absolutely one self indulgent ego trip!! #COP26   \n",
       "1  Please join us at a #COP26 side event \"Impleme...   \n",
       "2  #India‚Äôs huge solar uptake has boosted #climat...   \n",
       "3  Reuni√≥n #COP26 Reuni√≥n mundial de pa√≠ses sobre...   \n",
       "4  @_pln_id PLN terdepan dukung pemanfaatan EBTüçÉ\\...   \n",
       "\n",
       "                                    removed_hashtags  \\\n",
       "0          Absolutely one self indulgent ego trip!!    \n",
       "1  Please join us at a  side event \"Implementing ...   \n",
       "2  ‚Äôs huge solar uptake has boosted  goals, says ...   \n",
       "3  Reuni√≥n  Reuni√≥n mundial de pa√≠ses sobre cambi...   \n",
       "4  @_pln_id PLN terdepan dukung pemanfaatan EBTüçÉ\\...   \n",
       "\n",
       "                                          removed_av  \\\n",
       "0          Absolutely one self indulgent ego trip!!    \n",
       "1  Please join us at a  side event \"Implementing ...   \n",
       "2  ‚Äôs huge solar uptake has boosted  goals, says ...   \n",
       "3  Reuni√≥n  Reuni√≥n mundial de pa√≠ses sobre cambi...   \n",
       "4  @_pln_id PLN terdepan dukung pemanfaatan EBTüçÉ\\...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                 absolutely self indulgent ego trip   \n",
       "1  join event implement joint credit mechanism jc...   \n",
       "2         huge solar uptake boost goals say minister   \n",
       "3  reuni√≥n reuni√≥n mundial pa√≠ses sobre cambio cl...   \n",
       "4            pln pln terdepan dukung pemanfaatan ebt   \n",
       "\n",
       "                                          lemmatized  \n",
       "0                 absolutely self indulgent ego trip  \n",
       "1  join event implement joint credit mechanism jc...  \n",
       "2         huge solar uptake boost goals say minister  \n",
       "3  reuni√≥n reuni√≥n mundial pa√≠ses sobre cambio cl...  \n",
       "4            pln pln terdepan dukung pemanfaatan ebt  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "621c60d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lem_list = id_tweet_df['lemmatized'][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e1adc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['absolutely', 'self', 'indulgent', 'ego', 'trip']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lem_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ecb9a3",
   "metadata": {},
   "source": [
    "## Finding all the documents in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84b0291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 981 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/amirreza/opt/anaconda3/lib/python3.8/site-packages (from langdetect) (1.15.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=61823965434f640b418095642f1447f8f9a9b4bb46c2a15cedca36d7a28f1aa6\n",
      "  Stored in directory: /Users/amirreza/Library/Caches/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n",
    "import string\n",
    "from langdetect import detect\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bde8a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectlang(s):\n",
    "    try:\n",
    "        return detect(s)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c938449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "twitter_id                                                  OwlHoot16\n",
       "content             The CLIMATE CONFERENCE said #coal\\nIs no longe...\n",
       "removed_links       The CLIMATE CONFERENCE said #coal\\nIs no longe...\n",
       "removed_users       The CLIMATE CONFERENCE said #coal\\nIs no longe...\n",
       "removed_hashtags    The CLIMATE CONFERENCE said \\nIs no longer KIN...\n",
       "removed_av          The CLIMATE CONFERENCE said \\nIs no longer KIN...\n",
       "tokens              climate conference say longer king special lit...\n",
       "lemmatized          climate conference say longer king special lit...\n",
       "Name: 12, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tweet_df.loc[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa9d6495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "The CLIMATE CONFERENCE said #coal\n",
      "Is no longer KING and our very special little PRIME MINISTER SCOTT MORRISON said fuck...    #ClimateAction #COP26\n",
      "#Coal\n",
      "#Glasgow \n",
      "AUKUS \n",
      "#France \n",
      "#Submarines\n",
      "#Australia \n",
      "#auspol\n"
     ]
    }
   ],
   "source": [
    "content = id_tweet_df.loc[12].content\n",
    "print(detectlang(content))\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e131a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tweet_df['content'] = id_tweet_df.content.astype(str)\n",
    "id_tweet_df['lang'] = id_tweet_df['content'].map(detectlang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ee8bd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en       5954\n",
       "es        656\n",
       "fr        169\n",
       "nl        125\n",
       "it        119\n",
       "de        114\n",
       "cy         80\n",
       "pt         61\n",
       "ja         44\n",
       "id         33\n",
       "ca         32\n",
       "pl         29\n",
       "hi         27\n",
       "tr         19\n",
       "ar         18\n",
       "fi         13\n",
       "af         13\n",
       "ro         12\n",
       "tl         12\n",
       "et          8\n",
       "kn          7\n",
       "sv          6\n",
       "sw          6\n",
       "el          5\n",
       "ta          5\n",
       "fa          5\n",
       "ur          4\n",
       "cs          4\n",
       "ru          3\n",
       "ko          3\n",
       "hu          3\n",
       "gu          3\n",
       "no          3\n",
       "da          3\n",
       "th          3\n",
       "he          2\n",
       "hr          2\n",
       "so          2\n",
       "sl          2\n",
       "ne          2\n",
       "mr          1\n",
       "te          1\n",
       "vi          1\n",
       "zh-cn       1\n",
       "mk          1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tweet_df.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b9fa39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling only English article to reduce noise in NLP\n",
    "english_id_tweet_df = id_tweet_df[id_tweet_df.lang=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed7b4f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5954, 9)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_id_tweet_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d558c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_id_tweet_df.to_csv(\"english_clean_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bb96962d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitter_id</th>\n",
       "      <th>content</th>\n",
       "      <th>removed_links</th>\n",
       "      <th>removed_users</th>\n",
       "      <th>removed_hashtags</th>\n",
       "      <th>removed_av</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aquitainexox</td>\n",
       "      <td>@Banterm42404691 @MrHarryCole Absolutely one s...</td>\n",
       "      <td>@Banterm42404691 @MrHarryCole Absolutely one s...</td>\n",
       "      <td>Absolutely one self indulgent ego trip!! #COP26</td>\n",
       "      <td>Absolutely one self indulgent ego trip!!</td>\n",
       "      <td>Absolutely one self indulgent ego trip!!</td>\n",
       "      <td>absolutely self indulgent ego trip</td>\n",
       "      <td>absolutely self indulgent ego trip</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GEC_JCM_Info</td>\n",
       "      <td>Please join us at a #COP26 side event \"Impleme...</td>\n",
       "      <td>Please join us at a #COP26 side event \"Impleme...</td>\n",
       "      <td>Please join us at a #COP26 side event \"Impleme...</td>\n",
       "      <td>Please join us at a  side event \"Implementing ...</td>\n",
       "      <td>Please join us at a  side event \"Implementing ...</td>\n",
       "      <td>join event implement joint credit mechanism jc...</td>\n",
       "      <td>join event implement joint credit mechanism jc...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zpnine</td>\n",
       "      <td>#India‚Äôs huge solar uptake has boosted #climat...</td>\n",
       "      <td>#India‚Äôs huge solar uptake has boosted #climat...</td>\n",
       "      <td>#India‚Äôs huge solar uptake has boosted #climat...</td>\n",
       "      <td>‚Äôs huge solar uptake has boosted  goals, says ...</td>\n",
       "      <td>‚Äôs huge solar uptake has boosted  goals, says ...</td>\n",
       "      <td>huge solar uptake boost goals say minister</td>\n",
       "      <td>huge solar uptake boost goals say minister</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PGDynes</td>\n",
       "      <td>The concrete needed to accommodate 2 billion m...</td>\n",
       "      <td>The concrete needed to accommodate 2 billion m...</td>\n",
       "      <td>The concrete needed to accommodate 2 billion m...</td>\n",
       "      <td>The concrete needed to accommodate 2 billion m...</td>\n",
       "      <td>The concrete needed to accommodate 2 billion m...</td>\n",
       "      <td>concrete need accommodate billion people citie...</td>\n",
       "      <td>concrete need accommodate billion people citie...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Xplorit</td>\n",
       "      <td>#WestVirginia Senator &amp;amp; Ennersystams fraud...</td>\n",
       "      <td>#WestVirginia Senator &amp;amp; Ennersystams fraud...</td>\n",
       "      <td>#WestVirginia Senator &amp;amp; Ennersystams fraud...</td>\n",
       "      <td>Senator &amp;amp; Ennersystams fraud . gets a pro...</td>\n",
       "      <td>Senator &amp;amp; Ennersystams fraud . gets a pro...</td>\n",
       "      <td>senator amp ennersystams fraud get proper welc...</td>\n",
       "      <td>senator amp ennersystams fraud get proper welc...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>AqsaYounasRana</td>\n",
       "      <td>ClimateChange is no more only a challenge, it‚Äô...</td>\n",
       "      <td>ClimateChange is no more only a challenge, it‚Äô...</td>\n",
       "      <td>ClimateChange is no more only a challenge, it‚Äô...</td>\n",
       "      <td>ClimateChange is no more only a challenge, it‚Äô...</td>\n",
       "      <td>ClimateChange is no more only a challenge, it‚Äô...</td>\n",
       "      <td>climatechange challenge crisis soon catastroph...</td>\n",
       "      <td>climatechange challenge crisis soon catastroph...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>pete4peace</td>\n",
       "      <td>We confronted the coal baron this morning! Fuc...</td>\n",
       "      <td>We confronted the coal baron this morning! Fuc...</td>\n",
       "      <td>We confronted the coal baron this morning! Fuc...</td>\n",
       "      <td>We confronted the coal baron this morning! Fuc...</td>\n",
       "      <td>We confronted the coal baron this morning! Fuc...</td>\n",
       "      <td>confront coal baron morning fuck</td>\n",
       "      <td>confront coal baron morning fuck</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7613</th>\n",
       "      <td>mmill_landscape</td>\n",
       "      <td>Feeling simply disgusted by countries such as ...</td>\n",
       "      <td>Feeling simply disgusted by countries such as ...</td>\n",
       "      <td>Feeling simply disgusted by countries such as ...</td>\n",
       "      <td>Feeling simply disgusted by countries such as ...</td>\n",
       "      <td>Feeling simply disgusted by countries such as ...</td>\n",
       "      <td>feel simply disgust countries sign decrease st...</td>\n",
       "      <td>feel simply disgust countries sign decrease st...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7614</th>\n",
       "      <td>KarynCaplan</td>\n",
       "      <td>üö® Join us in calling for @Google and #Facebook...</td>\n",
       "      <td>üö® Join us in calling for @Google and #Facebook...</td>\n",
       "      <td>üö® Join us in calling for  and #Facebook () to ...</td>\n",
       "      <td>üö® Join us in calling for  and  () to stop prom...</td>\n",
       "      <td>üö® Join us in calling for  and  () to stop prom...</td>\n",
       "      <td>join call stop promote fund climate denial pub...</td>\n",
       "      <td>join call stop promote fund climate denial pub...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7615</th>\n",
       "      <td>DrDuly</td>\n",
       "      <td>There is no doubt! \"The science is clear: to k...</td>\n",
       "      <td>There is no doubt! \"The science is clear: to k...</td>\n",
       "      <td>There is no doubt! \"The science is clear: to k...</td>\n",
       "      <td>There is no doubt! \"The science is clear: to k...</td>\n",
       "      <td>There is no doubt! \"The science is clear: to k...</td>\n",
       "      <td>doubt science clear warm degrees cut global em...</td>\n",
       "      <td>doubt science clear warm degrees cut global em...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5954 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           twitter_id                                            content  \\\n",
       "0        aquitainexox  @Banterm42404691 @MrHarryCole Absolutely one s...   \n",
       "1        GEC_JCM_Info  Please join us at a #COP26 side event \"Impleme...   \n",
       "2              zpnine  #India‚Äôs huge solar uptake has boosted #climat...   \n",
       "5             PGDynes  The concrete needed to accommodate 2 billion m...   \n",
       "7             Xplorit  #WestVirginia Senator &amp; Ennersystams fraud...   \n",
       "...               ...                                                ...   \n",
       "7611   AqsaYounasRana  ClimateChange is no more only a challenge, it‚Äô...   \n",
       "7612       pete4peace  We confronted the coal baron this morning! Fuc...   \n",
       "7613  mmill_landscape  Feeling simply disgusted by countries such as ...   \n",
       "7614      KarynCaplan  üö® Join us in calling for @Google and #Facebook...   \n",
       "7615           DrDuly  There is no doubt! \"The science is clear: to k...   \n",
       "\n",
       "                                          removed_links  \\\n",
       "0     @Banterm42404691 @MrHarryCole Absolutely one s...   \n",
       "1     Please join us at a #COP26 side event \"Impleme...   \n",
       "2     #India‚Äôs huge solar uptake has boosted #climat...   \n",
       "5     The concrete needed to accommodate 2 billion m...   \n",
       "7     #WestVirginia Senator &amp; Ennersystams fraud...   \n",
       "...                                                 ...   \n",
       "7611  ClimateChange is no more only a challenge, it‚Äô...   \n",
       "7612  We confronted the coal baron this morning! Fuc...   \n",
       "7613  Feeling simply disgusted by countries such as ...   \n",
       "7614  üö® Join us in calling for @Google and #Facebook...   \n",
       "7615  There is no doubt! \"The science is clear: to k...   \n",
       "\n",
       "                                          removed_users  \\\n",
       "0       Absolutely one self indulgent ego trip!! #COP26   \n",
       "1     Please join us at a #COP26 side event \"Impleme...   \n",
       "2     #India‚Äôs huge solar uptake has boosted #climat...   \n",
       "5     The concrete needed to accommodate 2 billion m...   \n",
       "7     #WestVirginia Senator &amp; Ennersystams fraud...   \n",
       "...                                                 ...   \n",
       "7611  ClimateChange is no more only a challenge, it‚Äô...   \n",
       "7612  We confronted the coal baron this morning! Fuc...   \n",
       "7613  Feeling simply disgusted by countries such as ...   \n",
       "7614  üö® Join us in calling for  and #Facebook () to ...   \n",
       "7615  There is no doubt! \"The science is clear: to k...   \n",
       "\n",
       "                                       removed_hashtags  \\\n",
       "0             Absolutely one self indulgent ego trip!!    \n",
       "1     Please join us at a  side event \"Implementing ...   \n",
       "2     ‚Äôs huge solar uptake has boosted  goals, says ...   \n",
       "5     The concrete needed to accommodate 2 billion m...   \n",
       "7      Senator &amp; Ennersystams fraud . gets a pro...   \n",
       "...                                                 ...   \n",
       "7611  ClimateChange is no more only a challenge, it‚Äô...   \n",
       "7612  We confronted the coal baron this morning! Fuc...   \n",
       "7613  Feeling simply disgusted by countries such as ...   \n",
       "7614  üö® Join us in calling for  and  () to stop prom...   \n",
       "7615  There is no doubt! \"The science is clear: to k...   \n",
       "\n",
       "                                             removed_av  \\\n",
       "0             Absolutely one self indulgent ego trip!!    \n",
       "1     Please join us at a  side event \"Implementing ...   \n",
       "2     ‚Äôs huge solar uptake has boosted  goals, says ...   \n",
       "5     The concrete needed to accommodate 2 billion m...   \n",
       "7      Senator &amp; Ennersystams fraud . gets a pro...   \n",
       "...                                                 ...   \n",
       "7611  ClimateChange is no more only a challenge, it‚Äô...   \n",
       "7612  We confronted the coal baron this morning! Fuc...   \n",
       "7613  Feeling simply disgusted by countries such as ...   \n",
       "7614  üö® Join us in calling for  and  () to stop prom...   \n",
       "7615  There is no doubt! \"The science is clear: to k...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0                    absolutely self indulgent ego trip   \n",
       "1     join event implement joint credit mechanism jc...   \n",
       "2            huge solar uptake boost goals say minister   \n",
       "5     concrete need accommodate billion people citie...   \n",
       "7     senator amp ennersystams fraud get proper welc...   \n",
       "...                                                 ...   \n",
       "7611  climatechange challenge crisis soon catastroph...   \n",
       "7612                   confront coal baron morning fuck   \n",
       "7613  feel simply disgust countries sign decrease st...   \n",
       "7614  join call stop promote fund climate denial pub...   \n",
       "7615  doubt science clear warm degrees cut global em...   \n",
       "\n",
       "                                             lemmatized lang  \n",
       "0                    absolutely self indulgent ego trip   en  \n",
       "1     join event implement joint credit mechanism jc...   en  \n",
       "2            huge solar uptake boost goals say minister   en  \n",
       "5     concrete need accommodate billion people citie...   en  \n",
       "7     senator amp ennersystams fraud get proper welc...   en  \n",
       "...                                                 ...  ...  \n",
       "7611  climatechange challenge crisis soon catastroph...   en  \n",
       "7612                   confront coal baron morning fuck   en  \n",
       "7613  feel simply disgust countries sign decrease st...   en  \n",
       "7614  join call stop promote fund climate denial pub...   en  \n",
       "7615  doubt science clear warm degrees cut global em...   en  \n",
       "\n",
       "[5954 rows x 9 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_id_tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910dc13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
